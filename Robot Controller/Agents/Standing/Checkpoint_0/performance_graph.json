{"x_data": [1], "reward_mean": [17.39063589253032], "episode_length": [98.16201782226562], "policy_loss": [-0.15743261575698853], "value_loss": [48.184139251708984], "avg_action_time": [0.07430681586265564], "avg_time_between_actions": [1.1920928955078125e-07], "learning_rate": [103.38735415944569], "clip_fraction": [0], "current_x": 2}